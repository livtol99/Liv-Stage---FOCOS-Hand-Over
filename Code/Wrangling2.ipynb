{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have three data sets:\n",
    "- Markers' bios and metadata (markers_bios)\n",
    "- Followers' bios and metadata (followers_bios)\n",
    "- All brands and their followers (markers-followers)\n",
    "\n",
    "\n",
    "Step by step plan:\n",
    "1. Load the bios of followers, and the marker-follower file. \n",
    "    - Provide summary statistics of users and brands. How many brands do we have? How many followers? Any missing data, duplicates etc.?\n",
    "\n",
    "2. Filter on marker-follower df:\n",
    "    - Create a dictionary of counts brands per follower\n",
    "    - Remove users that follow less than 5 (or more) brands\n",
    "    - Continuously track numbers of users removed\n",
    "    - Match the Follower_Ids in the now filtered marker-follower df with the follower-bio df. As such, the follower bios will only include users that follow more than five brands. Subsequent filters will be on the correct users (up to date follower-bios).\n",
    "\n",
    "3. Do the filters on the follower-bios:\n",
    "    - Remove users with less than 25 followers\n",
    "    - Remove users with less than 100 tweets\n",
    "\n",
    "4. Filter based on language: keep only french accounts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import html\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "# Third-party library imports\n",
    "import dask.dataframe as dd\n",
    "from joblib import Parallel, delayed\n",
    "from langdetect import detect_langs, LangDetectException, DetectorFactory\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import pickle\n",
    "import regex\n",
    "import seaborn as sns\n",
    "from unidecode import unidecode\n",
    "import gcld3\n",
    "import ftfy\n",
    "import importlib\n",
    "\n",
    "# Local application/library specific imports\n",
    "import utils2\n",
    "from utils2 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load files and summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the data files and rename ID columns\n",
    "importlib.reload(utils2)\n",
    "\n",
    "# Load markers-followers\n",
    "load_path = '/home/livtollanes/SocialMarkers'\n",
    "file = 'markers_followers_2023-05-19.csv'\n",
    "\n",
    "req_cols = ['id', 'follower_id']\n",
    "dtypes = {'id': 'object',\n",
    "          'follower_id': 'object'}\n",
    "\n",
    "markers_followers = utils2.fileloader(load_path, file, req_cols, dtypes)\n",
    "\n",
    "\n",
    "#rename the twittwer id column to follower id \n",
    "markers_followers.rename(columns={'id':'marker_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the followers bios and rename ID columns\n",
    "load_path = '/home/livtollanes/SocialMarkers'\n",
    "file = 'markers_followers_bios_2023-05-19.csv'\n",
    "\n",
    "req_cols = ['twitter_id', 'id', 'screen_name', 'description', 'location', 'tweets', 'followers', 'friends', 'likes', 'lists','timestamp_utc']\n",
    "\n",
    "dtypes = {\n",
    "    'twitter_id': 'object',\n",
    "    'id': 'object',\n",
    "    'screen_name': 'object',\n",
    "    'description': 'object',\n",
    "    'location': 'object',\n",
    "    'tweets': 'float64',\n",
    "    'followers': 'float64',\n",
    "    'friends': 'float64',\n",
    "    'witheld_in_countries': 'float64'\n",
    "}\n",
    "\n",
    "followers_bios = utils2.fileloader(load_path, file, req_cols, dtypes)\n",
    "\n",
    "\n",
    "\n",
    "#rename the twittwer id column to follower id \n",
    "followers_bios.rename(columns={'twitter_id':'follower_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (70666646, 11)\n",
      "\n",
      "Columns in DataFrame:  ['follower_id', 'id', 'screen_name', 'description', 'timestamp_utc', 'location', 'tweets', 'followers', 'friends', 'likes', 'lists']\n",
      "\n",
      "Number of unique values in 'follower_id':  70666646\n",
      "Number of duplicate values in 'follower_id':  0\n",
      "\n",
      "Number of unique values in 'id':  70642661\n",
      "Number of duplicate values in 'id':  23984\n",
      "\n",
      "Number of missing values in each column:\n",
      "'follower_id':  0\n",
      "'id':  23985\n",
      "'screen_name':  23986\n",
      "'description':  42027215\n",
      "'timestamp_utc':  23985\n",
      "'location':  47956041\n",
      "'tweets':  23985\n",
      "'followers':  23985\n",
      "'friends':  23985\n",
      "'likes':  23985\n",
      "'lists':  23985\n",
      "\n",
      "Number of duplicate rows:  0\n"
     ]
    }
   ],
   "source": [
    "utils2.summary_stats(followers_bios, print_dtypes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (126345412, 2)\n",
      "\n",
      "Columns in DataFrame:  ['marker_id', 'follower_id']\n",
      "\n",
      "Number of unique values in 'follower_id':  70636295\n",
      "Number of duplicate values in 'follower_id':  55709117\n",
      "\n",
      "Number of unique values in 'marker_id':  236\n",
      "Number of duplicate values in 'marker_id':  126345176\n",
      "\n",
      "Number of missing values in each column:\n",
      "'marker_id':  0\n",
      "'follower_id':  0\n",
      "\n",
      "Number of duplicate rows:  2357493\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(utils2)\n",
    "utils2.summary_stats(markers_followers, print_dtypes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         marker_id          follower_id\n",
      "81032594  25053299            100000025\n",
      "89256298  25053299            100000025\n",
      "79323950  25053299  1000001004220420096\n",
      "87547795  25053299  1000001004220420096\n",
      "79369517  25053299  1000001771266232320\n",
      "87593356  25053299  1000001771266232320\n",
      "79562687  25053299           1000001815\n",
      "87786516  25053299           1000001815\n",
      "79371019  25053299  1000002790238777352\n",
      "87594858  25053299  1000002790238777352\n"
     ]
    }
   ],
   "source": [
    "# Sort the DataFrame to ensure that duplicates are next to each other\n",
    "markers_followers_sorted = markers_followers.sort_values(by=list(markers_followers.columns))\n",
    "\n",
    "# Find duplicates in the sorted DataFrame\n",
    "duplicates = markers_followers_sorted[markers_followers_sorted.duplicated(keep=False)]\n",
    "\n",
    "# Print the first 10 rows of duplicates (5 pairs)\n",
    "print(duplicates.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4714986, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the duplicates in markers_followers\n",
    "markers_followers.drop_duplicates(keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123987919, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markers_followers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 30351 unique values in df1 that don't exist in df2.\n",
      "There are 0 unique values in df2 that don't exist in df1.\n"
     ]
    }
   ],
   "source": [
    "compare_column_values(followers_bios, markers_followers, 'follower_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Filter the marker-follower df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Filter the marker-follower df:\n",
    "    - Remove users that follow less than 5 (or more) brands\n",
    "\n",
    "    - Continuously track numbers of users removed\n",
    "    \n",
    "    - Match the Follower_Ids in the now filtered marker-follower df with the follower-bio df. As such, the follower bios \n",
    "    will only include users that follow more than five brands. Subsequent filters will be on the correct users (up to date follower-bios)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove users that follow less than 5 brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66693204 followers follow less than 5 brands (94.42% of the total followers).\n",
      "After removing these followers, 3943091 followers are left (5.58% of the followers in the inputted df).\n"
     ]
    }
   ],
   "source": [
    "n = 5  # minimal number of brands followed required to be included in the analysis\n",
    "importlib.reload(utils2)\n",
    "markers_followers_5 = utils2.filter_followers(markers_followers, 'follower_id', n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed brands: {'1059975643'}\n"
     ]
    }
   ],
   "source": [
    "#number of unique brands left\n",
    "\n",
    "# Get the unique marker_id values in the original and filtered DataFrames\n",
    "original_brands = set(markers_followers['marker_id'].unique())\n",
    "filtered_brands = set(markers_followers_5['marker_id'].unique())\n",
    "\n",
    "# Find the brands that are in the original DataFrame but not in the filtered DataFrame\n",
    "removed_brands = original_brands - filtered_brands\n",
    "\n",
    "# Print the removed brands\n",
    "print(\"Removed brands:\", removed_brands) #corresponds to \"Napapijiri97\", which kindof sounds like a fake profile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match the IDs in the filtered marker-follower df with the follower bio df, so that the follower bios only are for those who follow at least 5 brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique follower_id in source DataFrame: 3943091\n",
      "Number of unique follower_id in filtered DataFrame after filtering: 3943091\n",
      "Removed 66723555 rows from the DataFrame to be filtered.\n",
      "3943091 rows are left in the filtered DataFrame.\n"
     ]
    }
   ],
   "source": [
    "followers_bios_5 = utils2.streamline_IDs(source = markers_followers_5, df_tofilter= followers_bios, 'follower_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 unique values in df1 that don't exist in df2.\n",
      "There are 0 unique values in df2 that don't exist in df1.\n"
     ]
    }
   ],
   "source": [
    "compare_column_values(followers_bios_5, markers_followers_5, 'follower_id')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Do the filters on the follower-bios:\n",
    "- Remove users with less than 25 followers\n",
    "- Remove users with less than 100 tweets\n",
    "- Update the markers-followers df to match the now filtered bio df\n",
    "- Filter based on language: keep only french accounts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 2750559 rows.\n",
      "1192532 rows are left.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "followers_bios_fullfilter = utils2.filter_by_tweets_and_followers(followers_bios_5, min_followers= 25, min_tweets= 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, remove the follower_Ids in markers-followers that don't occur in the newly filtered  followers_bios_nd5_tweets_followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique follower_id in source DataFrame: 1192532\n",
      "Number of unique follower_id in filtered DataFrame after filtering: 1192532\n",
      "Removed 18992709 rows from the DataFrame to be filtered.\n",
      "9614122 rows are left in the filtered DataFrame.\n"
     ]
    }
   ],
   "source": [
    "markers_followers_fullfilter = utils2.streamline_IDs(source= followers_bios_fullfilter, df_tofilter=markers_followers_5, column='follower_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 unique values in df1 that don't exist in df2.\n",
      "There are 0 unique values in df2 that don't exist in df1.\n"
     ]
    }
   ],
   "source": [
    "compare_column_values(followers_bios_fullfilter, markers_followers_fullfilter , 'follower_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before writing ti csv, clean description column to avoid writing problems\n",
    "importlib.reload(utils2)\n",
    "\n",
    "followers_bios_fullfilter = utils2.process_description(followers_bios_fullfilter, 'description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (9614122, 2)\n",
      "\n",
      "Columns in DataFrame:  ['marker_id', 'follower_id']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of unique values in 'follower_id':  1192532\n",
      "Number of duplicate values in 'follower_id':  8421590\n",
      "\n",
      "Number of unique values in 'marker_id':  235\n",
      "Number of duplicate values in 'marker_id':  9613887\n",
      "\n",
      "Number of missing values in each column:\n",
      "'marker_id':  0\n",
      "'follower_id':  0\n",
      "\n",
      "Number of duplicate rows:  0\n"
     ]
    }
   ],
   "source": [
    "summary_stats(markers_followers_fullfilter, print_dtypes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #Now write the two dfs to csvs to save them in case something happens\n",
    "markers_followers_fullfilter.to_csv('/home/livtollanes/NewData/markers_followers_cleaned_nolang.csv', encoding='utf-8', index=False)\n",
    "\n",
    "followers_bios_fullfilter.to_csv('/home/livtollanes/NewData/followers_bios_cleaned_nolang3.csv', sep=',', encoding='utf-8', index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Filter based on language: keep only french accounts\n",
    "- Use language recognition alorithms to filter the follower_bios. \n",
    "- We only want french language bios to be included\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import importlib\n",
    "\n",
    "from langdetect import detect_langs, LangDetectException, DetectorFactory\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from langutils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load marker followers\n",
    "full_path1 = '/home/livtollanes/NewData/markers_followers_cleaned_nolang.csv'\n",
    "req_cols = ['marker_id', 'follower_id']\n",
    "dtypes = {'marker_id': 'object',\n",
    "          'follower_id': 'object'}\n",
    "\n",
    "markers_followers_clean = pd.read_csv(full_path1, encoding='utf-8', dtype=dtypes, usecols=req_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Loading the followers bios (with cleaned description column)\n",
    "full_path = '/home/livtollanes/NewData/followers_bios_cleaned_nolang3.csv'\n",
    "\n",
    "req_cols = ['follower_id', 'screen_name', 'description', 'description_cleantext', 'location', 'tweets', 'followers', 'friends', 'likes', 'lists','timestamp_utc']\n",
    "\n",
    "dtypes = {\n",
    "    'follower_id': 'object',\n",
    "    'screen_name': 'object',\n",
    "    'description': 'object',\n",
    "    'description_cleantext': 'object',\n",
    "    'location': 'object',\n",
    "    'tweets': 'float64',\n",
    "    'followers': 'float64',\n",
    "    'friends': 'float64'\n",
    "}\n",
    "\n",
    "follower_bios_cleaned3 = pd.read_csv(full_path, usecols=req_cols, dtype=dtypes, engine= 'python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 unique values in df1 that don't exist in df2.\n",
      "There are 0 unique values in df2 that don't exist in df1.\n"
     ]
    }
   ],
   "source": [
    "compare_column_values(follower_bios_cleaned3, markers_followers_clean, 'follower_id')\n",
    "\n",
    "#The follower_ids are still streamlined, indicating that writing and reading of the cleaned dfs was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (1192532, 11)\n",
      "\n",
      "Columns in DataFrame:  ['follower_id', 'screen_name', 'description', 'timestamp_utc', 'location', 'tweets', 'followers', 'friends', 'likes', 'lists', 'description_cleantext']\n",
      "\n",
      "Number of unique values in 'follower_id':  1192532\n",
      "Number of duplicate values in 'follower_id':  0\n",
      "\n",
      "Number of missing values in each column:\n",
      "'follower_id':  0\n",
      "'screen_name':  0\n",
      "'description':  296871\n",
      "'timestamp_utc':  0\n",
      "'location':  376685\n",
      "'tweets':  0\n",
      "'followers':  0\n",
      "'friends':  0\n",
      "'likes':  0\n",
      "'lists':  0\n",
      "'description_cleantext':  313934\n",
      "\n",
      "Number of duplicate rows:  0\n"
     ]
    }
   ],
   "source": [
    "#should probably do summary stats on the dfs again, to make sure that no strange things have happened during the cleaning process\n",
    "summary_stats(follower_bios_cleaned3, print_dtypes= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (9614122, 2)\n",
      "\n",
      "Columns in DataFrame:  ['marker_id', 'follower_id']\n",
      "\n",
      "Number of unique values in 'follower_id':  1192532\n",
      "Number of duplicate values in 'follower_id':  8421590\n",
      "\n",
      "Number of unique values in 'marker_id':  235\n",
      "Number of duplicate values in 'marker_id':  9613887\n",
      "\n",
      "Number of missing values in each column:\n",
      "'marker_id':  0\n",
      "'follower_id':  0\n",
      "\n",
      "Number of duplicate rows:  0\n"
     ]
    }
   ],
   "source": [
    "summary_stats(markers_followers_clean, print_dtypes= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here comes the language detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First language detection is with the langdetect package. This package is based on Google's language detection API. On medium, it is actually reported that this package may not perform very accurate on short or mixed lnaguage texts, which is exactly what we are deailing with in twitter bios. \n",
    "- Gcld3 mmight be better at handling short text inputs like bios. By reading the documentation, gcld3 seems to be more fitting for social media data language detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(langutils)\n",
    "# Create a copy of the DataFrame for each function\n",
    "follower_bios_cleaned3_copy1 = follower_bios_cleaned3.copy()\n",
    "#follower_bios_cleaned3_copy2 = follower_bios_cleaned3.copy()\n",
    "\n",
    "# Use the copied DataFrames in the functions\n",
    "lang = langutils.add_and_detect_language(follower_bios_cleaned3_copy1, 'description_cleantext', seed = 3)\n",
    "#lang2 = utils2.detect_language_gcld3(follower_bios_cleaned3_copy2, 'description_cleantext', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['en', 'es', 'fr', 'unknown', 'de', 'tr', 'ca', 'ja', 'cy', 'it',\n",
       "       'no', 'pt', 'af', 'ar', 'el', 'et', 'so', 'ko', 'nl', 'pl', 'da',\n",
       "       'tl', 'sw', 'ru', 'sv', 'sk', 'ro', 'id', 'sl', 'hr', 'fi', 'hu',\n",
       "       'th', 'vi', 'mk', 'zh-cn', 'fa', 'hi', 'lt', 'ur', 'bn', 'lv',\n",
       "       'sq', 'cs', 'bg', 'ne', 'he', 'ta', 'uk', 'mr', 'gu', 'pa',\n",
       "       'zh-tw', 'ml', 'kn', 'te'], dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inspect the unique languages in lang\n",
    "lang['language'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French:  268891 ( 22.54790647127289 %)\n",
      "English:  353321 ( 29.627800344141708 %)\n",
      "Unknown:  327271 ( 27.443372588743948 %)\n",
      "Other:  243049 ( 20.380920595841467 %)\n",
      "NaN in description_cleantext:  313934 ( 26.324995891095586 %)\n"
     ]
    }
   ],
   "source": [
    "total_rows = lang.shape[0]\n",
    "\n",
    "french_rows = lang[lang['language'] == 'fr'].shape[0]\n",
    "english_rows = lang[lang['language'] == 'en'].shape[0]\n",
    "unknown_rows = lang[lang['language'] == 'unknown'].shape[0]\n",
    "NA_rows = lang[lang['language'] == 'NA'].shape[0]\n",
    "other_rows = total_rows - french_rows - english_rows - unknown_rows\n",
    "\n",
    "french_percent = (french_rows / total_rows) * 100\n",
    "english_percent = (english_rows / total_rows) * 100\n",
    "unknown_percent = (unknown_rows / total_rows) * 100\n",
    "other_percent = 100 - french_percent - english_percent - unknown_percent\n",
    "\n",
    "print(\"French: \", french_rows, \"(\", french_percent, \"%)\")\n",
    "print(\"English: \", english_rows, \"(\", english_percent, \"%)\")\n",
    "print(\"Unknown: \", unknown_rows, \"(\", unknown_percent, \"%)\")\n",
    "print(\"Other: \", other_rows, \"(\", other_percent, \"%)\")\n",
    "\n",
    "nan_rows = lang['description_cleantext'].isna().sum()\n",
    "nan_percent = (nan_rows / total_rows) * 100\n",
    "print(\"NaN in description_cleantext: \", nan_rows, \"(\", nan_percent, \"%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French:  20.72791338094072 %\n",
      "English:  21.173184451234853 %\n",
      "Unknown:  26.324995891095586 %\n",
      "Other:  31.77390627672884 %\n",
      "NaN in description_cleantext:  26.324995891095586 %\n"
     ]
    }
   ],
   "source": [
    "total_rows = lang2.shape[0]\n",
    "\n",
    "french_percent = (lang2[lang2['language'] == 'fr'].shape[0] / total_rows) * 100\n",
    "english_percent = (lang2[lang2['language'] == 'en'].shape[0] / total_rows) * 100\n",
    "unknown_percent = (lang2[lang2['language'] == 'unknown'].shape[0] / total_rows) * 100\n",
    "other_percent = 100 - french_percent - english_percent - unknown_percent\n",
    "\n",
    "print(\"French: \", french_percent, \"%\")\n",
    "print(\"English: \", english_percent, \"%\")\n",
    "print(\"Unknown: \", unknown_percent, \"%\")\n",
    "print(\"Other: \", other_percent, \"%\")\n",
    "\n",
    "nan_percent = (lang2['description_cleantext'].isna().sum() / total_rows) * 100\n",
    "print(\"NaN in description_cleantext: \", nan_percent, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually creating test sets to compare the two language detection methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the langdetect df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = lang.sample(n=100, random_state=1)\n",
    "test2 = lang2.sample(n=100, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "test1[['screen_name', 'location','description_cleantext', 'language']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "true_labels = {\n",
    "    'MorganJewelers1': 'non_fr',\n",
    "    'LeighAnnTowne': 'non_fr',\n",
    "    'ALBrutel': 'NA',\n",
    "    'Fonkwadiour1': 'non_fr',\n",
    "    'JUSTINAGROSSO': 'NA',\n",
    "    'SarahRadif': 'non_fr',\n",
    "    'CarineMayol': 'NA',\n",
    "    'ManonMondou': 'fr',\n",
    "    'erickleangar': 'NA',\n",
    "    'elaineguimarae': 'non_fr',\n",
    "    'danyunes': 'fr',\n",
    "    'kuronapilled': 'non_fr',\n",
    "    'KiriYaji': 'non_fr',\n",
    "    'floriannisolle': 'fr',\n",
    "    'Naman93726237': 'fr', #french but from guinea\n",
    "    'doktor_vananga': 'non_fr', \n",
    "    'Slaiidh': \"non_fr\", #wrote non. could be another language\n",
    "    'StarRaull': \"non_fr\",\n",
    "    'SPeperstraete': 'non_fr',\n",
    "    'yorgaine_lyon69': 'fr',\n",
    "    'blinameta': 'non_fr',\n",
    "    'AnnaRus75': 'non_fr',\n",
    "    'BernTurner': 'NA',\n",
    "    'lauriebeebe2': 'non_fr',\n",
    "    'JeremyyPalazzo': 'NA',\n",
    "    'GalerieCoulange': 'fr',\n",
    "    'HaziqkaJ': 'NA',\n",
    "    'DaraCheick': 'non_fr', #but seems to belong to a french uni\n",
    "    'mmdlbdrny2': 'NA',\n",
    "    'blandineleonie': 'fr',\n",
    "    'CarlosO324': 'non_fr',\n",
    "    'joshuatoney3': 'non_fr',\n",
    "    'StephaneWayler': 'fr',\n",
    "    'JessieV_214': 'non_fr',\n",
    "    'Apartofsuzanne': 'non_fr',#but writes that she is french, and location is Paris\n",
    "    'katuxka1': 'NA',\n",
    "    'tamaitanya': 'NA',\n",
    "    'sulagnasays': 'non_fr',\n",
    "    'nicolesimon': 'non_fr',\n",
    "    'lunasitah': 'non_fr',\n",
    "    'ln971': 'fr',\n",
    "    'olive_cas': 'non_fr',\n",
    "    'Mercedeszatfc': 'NA',\n",
    "    'hady_saad': 'non_fr',\n",
    "    'GatienLeroux': 'fr',\n",
    "    'Meluxiam': 'non_fr',\n",
    "    'yulimadera10': 'non_fr',\n",
    "    'RyanBrossault': 'fr',\n",
    "    'FlotillaMRY': 'non_fr',\n",
    "    'love98_daniela': 'non_fr',\n",
    "    'jucag4115': 'NA',\n",
    "    'Bxwiz': 'non_fr',\n",
    "    'MLECOMTE': 'fr',\n",
    "    'DelgadoJanahi': 'non_fr',\n",
    "    'f1rmin': 'fr',\n",
    "    'SylvainWalczak': 'NA',\n",
    "    'SoyLigiawn': 'non_fr',\n",
    "    'javielo26': 'non_fr',\n",
    "    'CherrybangMUA': 'non_fr',\n",
    "    '1927_albert': 'non_fr',\n",
    "    'aayyoud': 'NA',\n",
    "    'princessivy28': 'non_fr',\n",
    "    'drumz420': 'NA',\n",
    "    'Cjruin': 'non_fr',\n",
    "    'xXallymayXx': 'non_fr',\n",
    "    'NaAutacaace': 'fr',\n",
    "    'RosaLuc15360022': 'non_fr',\n",
    "    'PatrikWinston': 'non_fr',\n",
    "    'mwa_sisqo': 'NA',\n",
    "    'vankrug': 'NA',\n",
    "    'miktoi': 'non_fr',\n",
    "    'cescoeco': 'non_fr',\n",
    "    'floo_ncy': 'NA',\n",
    "    'Quentin_1411': 'non_fr',\n",
    "    'Tvy_Tk': 'NA',\n",
    "    'AlbanLeneveu': 'NA',\n",
    "    'adh2311': 'non_fr',\n",
    "    'loaizayose': 'NA',\n",
    "    'mathilde_Fparis': 'fr',\n",
    "    'gouillardmichae': 'fr',\n",
    "    'RomainSprynski': 'fr',\n",
    "    'styledscience': 'non_fr',\n",
    "    'amhammadi': 'NA',\n",
    "    'AliRazaSharif2': 'NA',\n",
    "    'JahanLutz': 'fr',\n",
    "    'HERVEJEHL': 'fr',\n",
    "    'CatherineMSch': 'fr',\n",
    "    'PoliKeyCo': 'non_fr',\n",
    "    'HisGraceth': 'non_fr',\n",
    "    'verstagoat': 'non_fr',\n",
    "    'G_deLinares': 'fr', \n",
    "    'Granadechkou': 'fr',\n",
    "    'MommaDandine': 'NA',\n",
    "    'allionesolution': 'non_fr',\n",
    "    'Ross75016': 'fr',\n",
    "    'DorignyTheo': 'NA',\n",
    "    'nikkolasg1': 'non_fr',\n",
    "    'InesNau': 'fr',\n",
    "    'FrenchEmperior': 'non_fr',\n",
    "    'BekaEssick': 'non_fr'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_language(lang):\n",
    "    if lang == 'unknown':\n",
    "        return 'NA'\n",
    "    elif lang == 'fr':\n",
    "        return 'fr'\n",
    "    else:\n",
    "        return 'non_fr'\n",
    "\n",
    "test1['pred_lang'] = test1['language'].apply(label_language)\n",
    "test2['pred_lang'] = test2['language'].apply(label_language)\n",
    "\n",
    "test1['true_lang'] = test1['screen_name'].map(true_labels)\n",
    "test2['true_lang'] = test2['screen_name'].map(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Calculate metrics for test1\n",
    "print(\"Metrics for test1:\")\n",
    "print(classification_report(test1['true_lang'], test1['pred_lang']))\n",
    "print(\"Accuracy:\", accuracy_score(test1['true_lang'], test1['pred_lang']))\n",
    "\n",
    "# Calculate metrics for test2\n",
    "print(\"\\nMetrics for test2:\")\n",
    "print(classification_report(test2['true_lang'], test2['pred_lang']))\n",
    "print(\"Accuracy:\", accuracy_score(test2['true_lang'], test2['pred_lang']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Still lack the actual language filtering\n",
    "- Think of a more sophisticated way to locate french users than only using language. This method should capture french users that write in english, and in french, and avoid selecting users that are writing in french but are not actually french. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIltering french users based on location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique locations: 231725 (19.4%)\n",
      "Users with location data: 815847 (68.4%)\n",
      "Users without location data: 376685 (31.6%)\n",
      "Users with bios: 878598 (73.7%)\n",
      "Users without bios: 313934 (26.3%)\n",
      "Users with both location and bios: 668947 (56.1%)\n"
     ]
    }
   ],
   "source": [
    "# Use the function\n",
    "utils2.location_bio_stats(follower_bios_cleaned3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a library called locationtagger. Might be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n"
     ]
    }
   ],
   "source": [
    "#check the type of the location column \n",
    "print(follower_bios_cleaned3['location'].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['follower_id', 'screen_name', 'description', 'timestamp_utc',\n",
       "       'location', 'tweets', 'followers', 'friends', 'likes', 'lists',\n",
       "       'description_cleantext'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "follower_bios_cleaned3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#switch to geotext environment for this to work\n",
    "# import geotext\n",
    "# from geotext import GeoText\n",
    "import nltk\n",
    "import spacy\n",
    "import locationtagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/livtollanes/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /home/livtollanes/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /home/livtollanes/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/treebank.zip.\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     /home/livtollanes/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/livtollanes/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/livtollanes/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.downloader.download('maxent_ne_chunker')\n",
    "nltk.downloader.download('words')\n",
    "nltk.downloader.download('treebank')\n",
    "nltk.downloader.download('maxent_treebank_pos_tagger')\n",
    "nltk.downloader.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m pool \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39mPool(mp\u001b[38;5;241m.\u001b[39mcpu_count())\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Use the map function of the Pool object to apply the function in parallel\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m follower_bios_cleaned3[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcountries\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextract_countries_locationtagger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollower_bios_cleaned3\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlocation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Close the pool to free up system resources\u001b[39;00m\n\u001b[1;32m     17\u001b[0m pool\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/geotext/lib/python3.11/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/geotext/lib/python3.11/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/geotext/lib/python3.11/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/geotext/lib/python3.11/threading.py:629\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    627\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 629\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniconda3/envs/geotext/lib/python3.11/threading.py:327\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import multiprocessing as mp\n",
    "\n",
    "# def extract_countries_locationtagger(text):\n",
    "#     if isinstance(text, str):\n",
    "#         locations = locationtagger.find_locations(text=text)\n",
    "#         return locations.countries\n",
    "#     else:\n",
    "#         return []\n",
    "\n",
    "# # Create a Pool object with the number of cores in your machine\n",
    "# pool = mp.Pool(mp.cpu_count())\n",
    "\n",
    "# # Use the map function of the Pool object to apply the function in parallel\n",
    "# follower_bios_cleaned3['countries'] = pool.map(extract_countries_locationtagger, follower_bios_cleaned3['location'])\n",
    "\n",
    "# # Close the pool to free up system resources\n",
    "# pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "# Take the first 100 rows of the DataFrame\n",
    "subset = follower_bios_cleaned3.iloc[200:300].copy()\n",
    "subset['location'] = subset['location'].str.lower()\n",
    "\n",
    "def extract_countries_locationtagger(text):\n",
    "    if isinstance(text, str):\n",
    "        if 'paris' in text or 'nice' in text or 'lyon' in text:\n",
    "            return ['france']\n",
    "        else:\n",
    "            locations = locationtagger.find_locations(text=text)\n",
    "            return locations.countries\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Open the Pool\n",
    "pool = multiprocessing.Pool()\n",
    "\n",
    "results = pool.map(extract_countries_locationtagger, subset['location'])\n",
    "subset.loc[:, 'countries'] = pd.Series(results, index=subset.index)\n",
    "\n",
    "# Close the Pool\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             location      countries\n",
      "200                            amiens             []\n",
      "201                               NaN             []\n",
      "202                            france       [france]\n",
      "203       casablanca-marseille-settat             []\n",
      "204                    southfield, mi             []\n",
      "205        duba√Ø, emirats arabes unis             []\n",
      "206                               NaN             []\n",
      "207            republica de venezuela             []\n",
      "208                               NaN             []\n",
      "209                            france       [france]\n",
      "210                               NaN             []\n",
      "211                 —Ä–æ—Å—Å–∏—è, –∞—Å—Ç—Ä–∞—Ö–∞–Ω—å             []\n",
      "212                          grenoble             []\n",
      "213                               NaN             []\n",
      "214                       bab ezzouar             []\n",
      "215          buenos aires - argentina             []\n",
      "216                               NaN             []\n",
      "217                 r√©gion parisienne       [france]\n",
      "218                      new york, ny             []\n",
      "219                worcester, england             []\n",
      "220                               NaN             []\n",
      "221                             tours             []\n",
      "222      denver|lima|bogota|usa|latam             []\n",
      "223                     paris, france       [france]\n",
      "224                               NaN             []\n",
      "225                           izanami             []\n",
      "226               digital town square             []\n",
      "227                               NaN             []\n",
      "228                               NaN             []\n",
      "229                    pantin, france       [france]\n",
      "230            yssingeaux, 43, france       [france]\n",
      "231                        martinique             []\n",
      "232                    paris, france        [france]\n",
      "233                    afghanistan üá¶üá´  [afghanistan]\n",
      "234                               NaN             []\n",
      "235                               NaN             []\n",
      "236                               NaN             []\n",
      "237                             paris       [france]\n",
      "238    west compton / los angeles, ca             []\n",
      "239                              lyon       [france]\n",
      "240                               NaN             []\n",
      "241                               NaN             []\n",
      "242                           ÿ™ŸàŸÜÿ≥ üáπüá≥             []\n",
      "243                              lyon       [france]\n",
      "244                     kumasi, ghana        [ghana]\n",
      "245                               NaN             []\n",
      "246                           kampala             []\n",
      "247            santiago de compostela             []\n",
      "248                     reims, troyes             []\n",
      "249                               NaN             []\n",
      "250                   zimbabwe africa     [zimbabwe]\n",
      "251                               NaN             []\n",
      "252                  catania, sicilia             []\n",
      "253                      florida, usa             []\n",
      "254              stormwind in azeroth             []\n",
      "255               north carolina, usa             []\n",
      "256                             paris       [france]\n",
      "257                          belgique             []\n",
      "258                            guin√©e             []\n",
      "259                            @peace             []\n",
      "260                               NaN             []\n",
      "261                             ahaha             []\n",
      "262             instagramakishafooney             []\n",
      "263                               NaN             []\n",
      "264                         guatemala             []\n",
      "265                             haiti        [haiti]\n",
      "266                  clermont-ferrand             []\n",
      "267                             italy        [italy]\n",
      "268  greater eccles, manchester, u.k.             []\n",
      "269               upper marlboro , md             []\n",
      "270                      bursa/turkey             []\n",
      "271              the village, belgium      [belgium]\n",
      "272                            beuvry             []\n",
      "273      cote d ivoire , abidjan 225              []\n",
      "274                               NaN             []\n",
      "275                               nyc             []\n",
      "276                               NaN             []\n",
      "277                          unknownüòú             []\n",
      "278                            annecy             []\n",
      "279               netherlaw, scotland             []\n",
      "280                               NaN             []\n",
      "281             venezuela- costa rica   [costa rica]\n",
      "282                               NaN             []\n",
      "283                     paris, france       [france]\n",
      "284                           algeria      [algeria]\n",
      "285                            france       [france]\n",
      "286                     paris, france       [france]\n",
      "287                   california, usa             []\n",
      "288                               NaN             []\n",
      "289                     paris, france       [france]\n",
      "290                               NaN             []\n",
      "291                               NaN             []\n",
      "292                               NaN             []\n",
      "293                               NaN             []\n",
      "294                         martigues             []\n",
      "295                          istanbul             []\n",
      "296                 tourcoing, france       [france]\n",
      "297                               NaN             []\n",
      "298                    abuja, nigeria             []\n",
      "299                               NaN             []\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "print(subset[['location', 'countries']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NER so far is not working very well.\n",
    "Try\n",
    "- bigger language models\n",
    "- more specific NER tagging tools for french locations\n",
    "- manually specify a list of the 100 biggest cities in France. set country to France if they match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "subset2 = follower_bios_cleaned3.head(100).copy()\n",
    "\n",
    "def extract_country_spacy(text):\n",
    "    if isinstance(text, str):\n",
    "        doc = nlp(text)\n",
    "        countries = [ent.text for ent in doc.ents if ent.label_ == 'GPE']\n",
    "        return countries\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "subset2['countries'] = subset['location'].apply(extract_country_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      location        countries\n",
      "0                          NaN               []\n",
      "1                    Venezuela      [Venezuela]\n",
      "2   Valencia _ Los guayos city               []\n",
      "3                          NaN               []\n",
      "4                          NaN               []\n",
      "5                          NaN               []\n",
      "6                      Espagne               []\n",
      "7                          NaN               []\n",
      "8                       France         [France]\n",
      "9                       France         [France]\n",
      "10                      France         [France]\n",
      "11                         NaN               []\n",
      "12                         NaN               []\n",
      "13         Bruxelles, Belgique               []\n",
      "14                    Shanghai       [Shanghai]\n",
      "15               Paris, France  [Paris, France]\n",
      "16               Blois, France  [Blois, France]\n",
      "17                       China          [China]\n",
      "18                      Berlin         [Berlin]\n",
      "19                         NaN               []\n"
     ]
    }
   ],
   "source": [
    "print(subset2[['location', 'countries']][0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, idetify country using NER from spaCy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spacy\n",
    "# python -m spacy download xx_ent_wiki_sm\n",
    "\n",
    "# import spacy\n",
    "\n",
    "# nlp = spacy.load('xx_ent_wiki_sm')\n",
    "\n",
    "# def extract_country_spacy(text):\n",
    "#     if isinstance(text, str):\n",
    "#         doc = nlp(text)\n",
    "#         # Extract entities that are countries\n",
    "#         countries = [ent.text for ent in doc.ents if ent.label_ == 'GPE']\n",
    "#         return countries\n",
    "#     else:\n",
    "#         return []\n",
    "    \n",
    "\n",
    "# df['country'] = df['location'].apply(extract_country_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>countries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Tourcoing, France</td>\n",
       "      <td>[France]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>C√¥te d'Ivoire</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>US: 28.534544,77.151007</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Paris</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>vienne</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>tunisia&amp;qatar</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Philippines</td>\n",
       "      <td>[Philippines]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Trujillo, Peru</td>\n",
       "      <td>[Peru]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Nashville, TN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Lagos Nigeria</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Paris La Mode</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>ici ailleurs ou autre part</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>California</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>La Rochelle, France</td>\n",
       "      <td>[France]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Acarigua</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Paris</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>7 bell yard london WC2A 2JR</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        location      countries\n",
       "80             Tourcoing, France       [France]\n",
       "81                 C√¥te d'Ivoire             []\n",
       "82       US: 28.534544,77.151007             []\n",
       "83                         Paris             []\n",
       "84                       vienne              []\n",
       "85                           NaN             []\n",
       "86                 tunisia&qatar             []\n",
       "87                   Philippines  [Philippines]\n",
       "88                Trujillo, Peru         [Peru]\n",
       "89                 Nashville, TN             []\n",
       "90                 Lagos Nigeria             []\n",
       "91                           NaN             []\n",
       "92                 Paris La Mode             []\n",
       "93    ici ailleurs ou autre part             []\n",
       "94                    California             []\n",
       "95           La Rochelle, France       [France]\n",
       "96                      Acarigua             []\n",
       "97                           NaN             []\n",
       "98                         Paris             []\n",
       "99                           NaN             []\n",
       "100  7 bell yard london WC2A 2JR             []"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect 'location' and 'countries' columns for rows from index 10 to 20\n",
    "follower_bios_cleaned3.loc[80:100, ['location', 'countries']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing the location search with lists of countries instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris 2138551\n",
      "Marseille 870731\n",
      "Lyon 522969\n",
      "Toulouse 493465\n",
      "Nice 342669\n",
      "Nantes 318808\n",
      "Strasbourg 274845\n",
      "Bordeaux 260958\n",
      "Montpellier 248252\n",
      "Rouen 234475\n",
      "Lille 234475\n",
      "Rennes 220488\n",
      "Reims 196565\n",
      "Le Havre 185972\n",
      "Cergy-Pontoise 183430\n",
      "Saint-√âtienne 176280\n",
      "Toulon 168701\n",
      "Angers 168279\n",
      "Grenoble 158552\n",
      "Dijon 158002\n",
      "N√Æmes 148236\n",
      "Clermont-Ferrand 147865\n",
      "Aix-en-Provence 146821\n",
      "Saint-Quentin-en-Yvelines 146598\n",
      "Brest 144899\n",
      "Le Mans 144515\n",
      "Amiens 143086\n",
      "Tours 141621\n",
      "Limoges 141176\n",
      "Villeurbanne 131445\n",
      "Besan√ßon 128426\n",
      "Metz 123914\n",
      "Orl√©ans 116269\n",
      "Mulhouse 111430\n",
      "Montreuil 111240\n",
      "Perpignan 110706\n",
      "Caen 110624\n",
      "Boulogne-Billancourt 108782\n",
      "Nancy 105058\n",
      "Lyon 03 102725\n",
      "Argenteuil 101475\n",
      "Roubaix 98828\n",
      "Tourcoing 98656\n",
      "Saint-Denis 96128\n",
      "Avignon 89769\n",
      "Marseille 13 89316\n",
      "Asni√®res-sur-Seine 86742\n",
      "Nanterre 86719\n",
      "Lyon 08 86154\n",
      "Poitiers 85960\n",
      "Versailles 85416\n",
      "Courbevoie 85158\n",
      "Cr√©teil 84833\n",
      "Pau 82697\n",
      "Lyon 07 82573\n",
      "Colombes 82300\n",
      "Vitry-sur-Seine 81001\n",
      "Aulnay-sous-Bois 80615\n",
      "Marseille 08 78837\n",
      "Marseille 15 77770\n",
      "Marseille 09 76868\n",
      "La Rochelle 76810\n",
      "Champigny-sur-Marne 76726\n",
      "Rueil-Malmaison 76616\n",
      "Antibes 76393\n",
      "Saint-Maur-des-Foss√©s 75402\n",
      "Cannes 74545\n",
      "Calais 74433\n",
      "B√©ziers 74081\n",
      "Dunkerque 71287\n",
      "Aubervilliers 70914\n",
      "M√©rignac 69791\n",
      "Bourges 67987\n",
      "Saint-Nazaire 67054\n",
      "Colmar 65405\n",
      "Valence 63864\n",
      "Quimper 63849\n",
      "Drancy 62488\n",
      "Noisy-le-Grand 62420\n",
      "Villeneuve-d'Ascq 62400\n",
      "La Seyne-sur-Mer 62330\n",
      "Levallois-Perret 62178\n",
      "Marseille 14 61920\n",
      "Chamb√©ry 61640\n",
      "Issy-les-Moulineaux 61447\n",
      "Neuilly-sur-Seine 61300\n",
      "Troyes 60785\n",
      "Antony 59845\n",
      "La Roche-sur-Yon 59410\n",
      "Marseille 12 58734\n",
      "Lorient 58112\n",
      "Sarcelles 57979\n",
      "Pessac 57944\n",
      "Ivry-sur-Seine 57897\n",
      "√âvreux 57795\n",
      "V√©nissieux 57584\n",
      "Cergy 57576\n",
      "Clichy 57467\n",
      "Marseille 11 56792\n",
      "Saint-Quentin 55407\n",
      "Niort 54660\n",
      "Belfort 54562\n",
      "Ajaccio 54364\n",
      "Vannes 54020\n",
      "Sartrouville 53980\n",
      "Maisons-Alfort 53964\n",
      "Meaux 53811\n",
      "Blois 53660\n",
      "Brive-la-Gaillarde 53466\n",
      "Arles 53431\n",
      "Beauvais 53393\n",
      "Ch√¢teauroux 53301\n",
      "Cholet 53160\n",
      "Fr√©jus 53098\n",
      "Pantin 52922\n",
      "Lyon 06 52862\n",
      "Saint-Brieuc 52774\n",
      "Montauban 52434\n",
      "Charleville-M√©zi√®res 52415\n",
      "Albi 52409\n",
      "Tarbes 52106\n",
      "Fontenay-sous-Bois 52075\n",
      "Lyon 09 51983\n",
      "√âvry 51900\n",
      "Clamart 51400\n",
      "Marseille 10 51299\n",
      "Ch√¢lons-en-Champagne 51257\n",
      "Narbonne 50776\n",
      "Saint-Malo 50676\n",
      "Laval 50489\n",
      "Hy√®res 50487\n",
      "Lyon 05 49664\n",
      "Carcassonne 49600\n",
      "Angoul√™me 49468\n",
      "Annecy 49232\n",
      "Bondy 48268\n",
      "Le Blanc-Mesnil 48077\n",
      "Villejuif 48048\n",
      "Grasse 47581\n",
      "Sevran 47334\n",
      "Castres 47275\n",
      "Chalon-sur-Sa√¥ne 47251\n",
      "Marseille 04 47193\n",
      "Arras 47052\n",
      "Boulogne-sur-Mer 47013\n",
      "Chelles 46947\n",
      "Cagnes-sur-Mer 46923\n",
      "Saint-Herblain 46898\n",
      "Douai 46531\n",
      "√âpinay-sur-Seine 46145\n",
      "Vincennes 45923\n",
      "Martigues 45749\n",
      "Marseille 03 45414\n",
      "Bourg-en-Bresse 45340\n",
      "Bobigny 44962\n",
      "Montlu√ßon 44960\n",
      "Aubagne 44844\n",
      "Valenciennes 44821\n",
      "Suresnes 44697\n",
      "Meudon 44652\n",
      "Marseille 05 44583\n",
      "Bayonne 44396\n",
      "Istres 44373\n",
      "Thionville 44311\n",
      "Mantes-la-Jolie 44263\n",
      "Compi√®gne 44243\n",
      "Nevers 43988\n",
      "Chartres 43838\n",
      "Marseille 06 43360\n",
      "Le Cannet 43353\n",
      "Wattrelos 42826\n",
      "Caluire-et-Cuire 42763\n",
      "Gap 42715\n",
      "Talence 42579\n",
      "Al√®s 42410\n",
      "Gennevilliers 42294\n",
      "Saint-Priest 41641\n",
      "Rosny-sous-Bois 41627\n",
      "Auxerre 41516\n",
      "Salon-de-Provence 41397\n",
      "Saint-Germain-en-Laye 41142\n",
      "Bastia 41001\n",
      "Marseille 01 40919\n",
      "S√®te 40736\n",
      "Anglet 40658\n",
      "Puteaux 40646\n",
      "Corbeil-Essonnes 40527\n",
      "Garges-l√®s-Gonesse 39847\n",
      "Roanne 39840\n",
      "Savigny-sur-Orge 39698\n",
      "Livry-Gargan 39518\n",
      "√âpinal 39518\n",
      "Saint-Ouen 39353\n",
      "Rez√© 39248\n",
      "Montigny-le-Bretonneux 39063\n",
      "Noisy-le-Sec 38955\n",
      "Melun 38953\n",
      "Bagneux 38900\n",
      "Massy 38768\n",
      "Haguenau 38721\n",
      "Montrouge 38708\n",
      "Marcq-en-Bar≈ìul 38629\n",
      "Vitrolles 38591\n",
      "Draguignan 38573\n",
      "Lens 38265\n",
      "Gagny 38134\n",
      "Saint-Chamond 38014\n",
      "Bron 37825\n",
      "Vaulx-en-Velin 37489\n",
      "Jou√©-l√®s-Tours 37466\n",
      "Alfortville 37290\n",
      "La Courneuve 37287\n",
      "Ch√¢tellerault 37210\n",
      "Villepinte 37114\n",
      "Cambrai 36492\n",
      "Poissy 36431\n",
      "Conflans-Sainte-Honorine 36358\n",
      "Mont-de-Marsan 36205\n",
      "Plaisir 36121\n",
      "Lyon 04 36064\n",
      "Marseille 07 35981\n",
      "Marignane 35873\n",
      "Dieppe 35707\n",
      "Mont√©limar 35637\n",
      "Tremblay-en-France 35591\n",
      "Choisy-le-Roi 35590\n",
      "Saint-Martin-d'H√®res 35528\n",
      "M√¢con 35484\n",
      "Romans-sur-Is√®re 35002\n",
      "Saint-Rapha√´l 34918\n",
      "Pontault-Combault 34798\n",
      "Six-Fours-les-Plages 34779\n",
      "P√©rigueux 34770\n",
      "Aurillac 34724\n",
      "Neuilly-sur-Marne 34465\n",
      "Li√©vin 34370\n",
      "Agen 34367\n",
      "Franconville 34284\n",
      "Sainte-Genevi√®ve-des-Bois 33689\n",
      "Maubeuge 33684\n",
      "Bagnolet 33504\n",
      "Saumur 33229\n",
      "Biarritz 33188\n",
      "La Ciotat 33113\n",
      "√âchirolles 33088\n",
      "Villefranche-sur-Sa√¥ne 32994\n",
      "Ch√¢tenay-Malabry 32715\n",
      "Stains 32601\n",
      "Ch√¢tillon 32383\n",
      "Vienne 32293\n",
      "Schiltigheim 32289\n",
      "Les Mureaux 32134\n",
      "Le Perreux-sur-Marne 32015\n",
      "Guyancourt 31989\n",
      "Palaiseau 31987\n",
      "Saint-Dizier 31918\n",
      "Creil 31863\n",
      "Vand≈ìuvre-l√®s-Nancy 31785\n",
      "Thonon-les-Bains 31684\n",
      "B√©thune 31568\n",
      "Colomiers 31363\n",
      "Lyon 02 31303\n",
      "Nogent-sur-Marne 31236\n",
      "Athis-Mons 31225\n",
      "Viry-Ch√¢tillon 31131\n",
      "Houilles 31121\n",
      "Soissons 31100\n",
      "Dreux 31058\n",
      "Charenton-le-Pont 30910\n",
      "Villeneuve-Saint-Georges 30881\n",
      "√âlancourt 30831\n",
      "Alen√ßon 30786\n",
      "Sotteville-l√®s-Rouen 30619\n",
      "Orange 30530\n",
      "Villenave-d'Ornon 30347\n",
      "Pontoise 30290\n",
      "Villiers-sur-Marne 30252\n",
      "Chatou 30091\n",
      "Goussainville 30047\n",
      "Lomme 29892\n",
      "Vierzon 29876\n",
      "Saint-M√©dard-en-Jalles 29742\n",
      "Thiais 29724\n",
      "Carpentras 29709\n",
      "L'Ha√ø-les-Roses 29703\n",
      "Menton 29649\n",
      "Lyon 01 29641\n",
      "Saint-Laurent-du-Var 29516\n",
      "Saintes 29512\n",
      "Rochefort 29427\n",
      "Malakoff 29420\n",
      "Draveil 29316\n",
      "Saint-Leu 29278\n",
      "Saint-S√©bastien-sur-Loire 29238\n",
      "Le Chesnay 29154\n",
      "Tournefeuille 29124\n",
      "Clichy-sous-Bois 29062\n",
      "Chaumont 28981\n",
      "Saint-√âtienne-du-Rouvray 28953\n",
      "Meyzieu 28929\n",
      "Yerres 28897\n",
      "Montb√©liard 28865\n",
      "Saint-Cloud 28839\n",
      "Sens 28700\n",
      "Laon 28688\n",
      "Lambersart 28613\n",
      "Trappes 28367\n",
      "Rodez 28337\n",
      "Bergerac 28317\n",
      "Annemasse 28275\n",
      "La Garde 28257\n",
      "Ermont 28117\n",
      "Rillieux-la-Pape 28076\n",
      "Villemomble 28001\n",
      "Pierrefitte-sur-Seine 27914\n",
      "Le Kremlin-Bic√™tre 27867\n",
      "Champs-sur-Marne 27776\n",
      "Aix-les-Bains 27651\n",
      "Bezons 27570\n",
      "Vallauris 27286\n",
      "Taverny 27271\n",
      "Illkirch-Graffenstaden 27261\n",
      "La Teste-de-Buch 27253\n",
      "√âpernay 27082\n",
      "Villiers-le-Bel 27028\n",
      "Vichy 27019\n",
      "Sannois 26869\n",
      "Gonesse 26758\n",
      "Vigneux-sur-Seine 26692\n",
      "Cavaillon 26689\n",
      "Rambouillet 26674\n",
      "Cherbourg-Octeville 26655\n",
      "Armenti√®res 26646\n",
      "Lunel 26588\n",
      "Cachan 26540\n",
      "Le Grand-Quevilly 26522\n",
      "Abbeville 26461\n",
      "La Garenne-Colombes 26385\n",
      "Montigny-l√®s-Metz 26369\n",
      "Vanves 26068\n",
      "Vernon 26037\n",
      "Savigny-le-Temple 25925\n",
      "Dole 25878\n",
      "Villeneuve-sur-Lot 25869\n",
      "Les Ulis 25785\n",
      "Marseille 02 25779\n",
      "Oyonnax 25697\n",
      "Oullins 25592\n",
      "Le Creusot 25590\n",
      "H√©nin-Beaumont 25371\n",
      "Orvault 25338\n",
      "Coudekerque-Branche 25201\n",
      "Herblay 25153\n",
      "Ris-Orangis 25082\n",
      "Sucy-en-Brie 25014\n",
      "Grigny 24940\n",
      "Fresnes 24803\n",
      "Romainville 24772\n",
      "Fontenay-aux-Roses 24680\n",
      "D√©cines-Charpieu 24674\n",
      "Saint-Di√©-des-Vosges 24628\n",
      "Fontaine 24588\n",
      "Auch 24494\n",
      "Bruay-la-Buissi√®re 24474\n",
      "Lisieux 24473\n",
      "Sarreguemines 24446\n",
      "Torcy 24386\n",
      "Gradignan 24385\n",
      "Lanester 24352\n",
      "Br√©tigny-sur-Orge 24317\n",
      "Muret 24313\n",
      "Bois-Colombes 24300\n",
      "Blagnac 24263\n",
      "H√©rouville-Saint-Clair 24258\n",
      "Libourne 24240\n",
      "Le Bouscat 24232\n",
      "Miramas 24173\n",
      "Beaune 24162\n",
      "Eaubonne 24096\n",
      "Brunoy 24096\n",
      "Villeparisis 23931\n",
      "Montfermeil 23829\n",
      "S√®vres 23724\n",
      "Foug√®res 23719\n",
      "Saint-Pol-sur-Mer 23635\n",
      "La Madeleine 23572\n",
      "Gif-sur-Yvette 23541\n",
      "Bourgoin-Jallieu 23517\n",
      "B√®gles 23504\n",
      "Cahors 23331\n",
      "Millau 23307\n",
      "Hazebrouck 23307\n",
      "Grande-Synthe 23213\n",
      "La Valette-du-Var 23134\n",
      "Moulins 23095\n",
      "Mont-Saint-Aignan 23078\n",
      "√âtampes 23012\n",
      "Mons-en-Bar≈ìul 23006\n",
      "Agde 23001\n",
      "Saint-Ouen-l'Aum√¥ne 22977\n",
      "Vertou 22921\n",
      "Montgeron 22843\n",
      "Maisons-Laffitte 22772\n",
      "Le Puy-en-Velay 22718\n",
      "Le Petit-Quevilly 22691\n",
      "Lormont 22636\n",
      "Olivet 22604\n",
      "Ach√®res 22555\n",
      "Ozoir-la-Ferri√®re 22530\n",
      "Le Plessis-Robinson 22510\n",
      "Cenon 22393\n",
      "Combs-la-Ville 22322\n",
      "Dax 22305\n",
      "Frontignan 22251\n",
      "Voiron 22114\n",
      "Loos 22107\n",
      "Le M√©e-sur-Seine 22058\n",
      "Villeneuve-la-Garenne 22036\n",
      "F√©camp 22003\n",
      "Cormeilles-en-Parisis 21973\n",
      "Sainte-Foy-l√®s-Lyon 21893\n",
      "Verdun 21798\n",
      "V√©lizy-Villacoublay 21741\n",
      "Saint-L√¥ 21722\n",
      "Lun√©ville 21582\n",
      "Deuil-la-Barre 21560\n",
      "La Celle-Saint-Cloud 21539\n",
      "Annecy-le-Vieux 21521\n",
      "Sceaux 21511\n",
      "Lannion 21473\n",
      "Denain 21412\n",
      "Allauch 21406\n",
      "Concarneau 21397\n",
      "Eysines 21394\n",
      "Croix 21361\n",
      "Forbach 21358\n",
      "Manosque 21351\n",
      "Saint-Mand√© 21261\n",
      "Montmorency 21156\n",
      "Gardanne 21140\n",
      "Les Lilas 21124\n",
      "Fleury-les-Aubrais 21085\n",
      "Dammarie-les-Lys 21066\n",
      "Roissy-en-Brie 20933\n",
      "Saint-Genis-Laval 20883\n",
      "Saint-Louis 20871\n",
      "Cognac 20868\n",
      "Saint-Michel-sur-Orge 20824\n",
      "Longjumeau 20771\n",
      "Bressuire 20743\n",
      "Vaur√©al 20699\n",
      "Lons-le-Saunier 20678\n",
      "Ploemeur 20652\n",
      "Lagny-sur-Marne 20649\n",
      "Sedan 20620\n",
      "Orly 20528\n",
      "Pontarlier 20313\n",
      "Chamali√®res 20298\n",
      "Meylan 20267\n",
      "Montceau-les-Mines 20221\n",
      "Halluin 20171\n",
      "Riom 20142\n",
      "Chevilly-Larue 20125\n",
      "La Defense 20000\n",
      "Sainte-Marguerite 19997\n",
      "Romorantin-Lanthenay 19953\n",
      "Mantes-la-Ville 19947\n",
      "Pertuis 19926\n",
      "Morsang-sur-Orge 19917\n",
      "Vesoul 19892\n",
      "Saint-Jean-de-Braye 19874\n",
      "Les Pennes-Mirabeau 19871\n",
      "Cluses 19789\n",
      "Bourg-l√®s-Valence 19770\n",
      "Villefontaine 19749\n",
      "Saint-Gratien 19737\n",
      "Les Pavillons-sous-Bois 19730\n",
      "Fontainebleau 19717\n",
      "Hem 19699\n",
      "Nogent-sur-Oise 19690\n",
      "Bagnols-sur-C√®ze 19640\n",
      "Cou√´ron 19466\n",
      "√âcully 19437\n",
      "Cran-Gevrier 19354\n",
      "Wasquehal 19320\n",
      "Mandelieu-la-Napoule 19292\n",
      "Vend√¥me 19226\n",
      "Louviers 19220\n",
      "Gujan-Mestras 19184\n",
      "Sanary-sur-Mer 19166\n",
      "Senlis 19160\n",
      "Albertville 19113\n",
      "Limeil-Br√©vannes 19104\n",
      "Jouy-le-Moutier 19087\n",
      "Marmande 19069\n",
      "Royan 19017\n",
      "Maurepas 19009\n",
      "Digne-les-Bains 18986\n",
      "Mougins 18973\n",
      "Challans 18947\n",
      "S√©lestat 18941\n",
      "Garches 18930\n",
      "Chilly-Mazarin 18870\n",
      "Bourg-la-Reine 18870\n",
      "√âqueurdreville-Hainneville 18862\n",
      "Cournon-d'Auvergne 18848\n",
      "Chaville 18735\n",
      "Neuilly-Plaisance 18725\n",
      "Bonneuil-sur-Marne 18723\n",
      "Sorgues 18681\n",
      "Vence 18668\n",
      "Saint-Barth√©l√©my 18655\n",
      "Le Plessis-Tr√©vise 18618\n",
      "Bar-le-Duc 18595\n",
      "Seynod 18590\n",
      "Mitry-Mory 18562\n",
      "Carvin 18561\n",
      "Limay 18559\n",
      "La Fl√®che 18536\n",
      "Plaisance-du-Touch 18501\n",
      "La Chapelle-sur-Erdre 18481\n",
      "Avion 18470\n",
      "Annonay 18423\n",
      "Ronchin 18331\n",
      "Mauguio 18320\n",
      "Chennevi√®res-sur-Marne 18314\n",
      "Autun 18283\n",
      "Saint-Avold 18281\n",
      "Tourlaville 18273\n",
      "Saint-Omer 18250\n",
      "Argentan 18230\n",
      "Givors 18198\n",
      "Joinville-le-Pont 18154\n",
      "Carquefou 18056\n",
      "Cestas 18036\n",
      "Castelnau-le-Lez 18011\n",
      "Montigny-l√®s-Cormeilles 17910\n",
      "La Crau 17905\n",
      "Firminy 17902\n",
      "Orsay 17817\n",
      "Saint-Amand-les-Eaux 17808\n",
      "La Pomme 17787\n",
      "Le Pr√©-Saint-Gervais 17786\n",
      "Luc√© 17780\n",
      "Les Clayes-sous-Bois 17776\n",
      "La Baule-Escoublac 17775\n",
      "Sin-le-Noble 17682\n",
      "Toul 17680\n",
      "Marseille 16 17630\n",
      "Villeneuve-le-Roi 17575\n",
      "Mazargues 17527\n",
      "Saint-Jean-de-la-Ruelle 17522\n",
      "Morlaix 17516\n",
      "Montargis 17457\n",
      "Soisy-sous-Montmorency 17436\n",
      "Bischheim 17434\n",
      "Flers 17430\n",
      "Marly-le-Roi 17404\n",
      "Lattes 17390\n",
      "Saint-Cyr-sur-Loire 17337\n",
      "Fos-sur-Mer 17317\n",
      "Arcueil 17308\n",
      "Lingolsheim 17284\n",
      "Vitr√© 17266\n",
      "Vitry-le-Fran√ßois 17250\n",
      "Elbeuf 17231\n",
      "Pamiers 16997\n",
      "Montereau-Fault-Yonne 16993\n",
      "Tulle 16969\n",
      "L'Isle-sur-la-Sorgue 16968\n",
      "Octeville 16951\n",
      "Tassin-la-Demi-Lune 16920\n",
      "Bouguenais 16824\n",
      "Verneuil-sur-Seine 16790\n",
      "Verri√®res-le-Buisson 16789\n",
      "Pontivy 16752\n",
      "Le V√©sinet 16740\n",
      "Sallanches 16725\n",
      "Montivilliers 16706\n",
      "Domont 16684\n",
      "Blanquefort 16636\n",
      "Douarnenez 16590\n",
      "Le Camas 16585\n",
      "Osny 16545\n",
      "Le Pecq 16534\n",
      "Gu√©rande 16523\n",
      "Floirac 16497\n",
      "Saint-Cyr-l'√âcole 16365\n",
      "Laxou 16323\n",
      "Fontenay-le-Comte 16316\n",
      "Moissy-Cramayel 16259\n",
      "Saran 16243\n",
      "Bry-sur-Marne 16226\n",
      "Cesson-S√©vign√© 16222\n",
      "Le Pontet 16182\n",
      "Montbrison 16164\n",
      "Faches-Thumesnil 16163\n",
      "Viroflay 16137\n",
      "Les Sables-d'Olonne 16105\n",
      "Saint-Loup 16084\n",
      "Saint-Leu-la-For√™t 16075\n",
      "Saint-Fons 16053\n",
      "Landerneau 16052\n",
      "Saint-Avertin 16007\n",
      "Ch√¢teaudun 16006\n",
      "Issoire 15984\n",
      "Gien 15966\n",
      "Bayeux 15963\n",
      "Gentilly 15939\n",
      "Ch√¢teau-Thierry 15938\n",
      "Port-de-Bouc 15934\n",
      "Saint-√âgr√®ve 15904\n",
      "Villers-l√®s-Nancy 15899\n",
      "Brie-Comte-Robert 15892\n",
      "Boissy-Saint-L√©ger 15877\n",
      "Gu√©ret 15853\n",
      "Chen√¥ve 15791\n",
      "Lourdes 15786\n",
      "Wittenheim 15747\n",
      "Canteleu 15742\n",
      "Haubourdin 15694\n",
      "Cr√©py-en-Valois 15694\n",
      "Berck-Plage 15675\n",
      "Les Herbiers 15664\n",
      "Yutz 15659\n",
      "Outreau 15648\n",
      "Berck 15609\n",
      "Mayenne 15583\n",
      "Audincourt 15577\n",
      "Balma 15553\n",
      "Hautmont 15540\n",
      "Lognes 15519\n",
      "Hayange 15501\n",
      "Tergnier 15475\n",
      "Carri√®res-sous-Poissy 15465\n",
      "Illzach 15457\n",
      "Pont-√†-Mousson 15411\n",
      "Cugnaux 15393\n",
      "√âragny 15385\n",
      "Noisiel 15362\n",
      "La Blancarde 15292\n",
      "Saint-Pierre-des-Corps 15284\n",
      "Coulommiers 15274\n",
      "Montesson 15256\n",
      "Saint-Maximin-la-Sainte-Baume 15225\n",
      "Les Olives 15181\n",
      "Noyon 15138\n",
      "Brignoles 15027\n",
      "Avon 15009\n"
     ]
    }
   ],
   "source": [
    "import geonamescache\n",
    "\n",
    "gc = geonamescache.GeonamesCache()\n",
    "all_cities = gc.get_cities()\n",
    "\n",
    "# Filter cities by country code\n",
    "cities = {k: v for k, v in all_cities.items() if v['countrycode'] == 'FR'}\n",
    "\n",
    "# Convert the cities dictionary to a list of tuples\n",
    "cities_list = [(city['name'], city['population']) for city in cities.values()]\n",
    "\n",
    "# Sort the list by population in descending order and take the first 100\n",
    "biggest_cities = sorted(cities_list, key=lambda x: x[1], reverse=True)[:650]\n",
    "\n",
    "# Print the 100 biggest cities\n",
    "for city, population in biggest_cities:\n",
    "    print(city, population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "649"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(biggest_cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geotext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
